{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4351a728",
   "metadata": {},
   "source": [
    "# Spark streaming\n",
    "\n",
    "[Documentstion](https://spark.apache.org/docs/latest/streaming-programming-guide.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8d0166",
   "metadata": {},
   "source": [
    "## Window Operations Types\n",
    "\n",
    "### window\n",
    "Compute new DStream based on windowed batches of source DS.\n",
    "\n",
    "### countByWindow\n",
    "Sliding window count of elements.\n",
    "\n",
    "### reduceByKeyAndWindow\n",
    "Applied on PairDStreams. Values for each key are aggregated over a sliding window. Improve performance with inverse function.\n",
    "\n",
    "### countByValueAndWindow\n",
    "Returns DStream of (K,V} pairs where the value of each key is its frequency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c885b2f",
   "metadata": {},
   "source": [
    "## Quiz:\n",
    "\n",
    "- Which operation returns a new single-element stream, created by aggregating elements in the stream over a sliding interval using func?\n",
    "    - reduceByWindow\n",
    "\n",
    "\n",
    "- What is Spark Streaming?\n",
    "    - It is an extension of the core Spark API that enables scalable, high-throughput, fault-tolerance stream processing of live data streams.\n",
    "\n",
    "\n",
    "- What stateful operations exist?\n",
    "    - updateStateByKey\n",
    "    - mapWithState\n",
    "\n",
    "\n",
    "- You cannot develop custom streaming sources for your own needs in Spark Streaming.\n",
    "    - False\n",
    "\n",
    "\n",
    "- SparkContext is the entry point for all spark streaming functionality.\n",
    "    - False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0709ad",
   "metadata": {},
   "source": [
    "## Spark Data Streaming\n",
    "\n",
    "### Quiz:\n",
    "\n",
    "- What is true about streaming data?\n",
    "    - Streaming data is generated continuously by data sources\n",
    "    - Wide variety of sources\n",
    "    - Data is processed sequentially\n",
    "\n",
    "\n",
    "- In what way is streaming implemented in spark streaming?\n",
    "    - Micro batching\n",
    "\n",
    "\n",
    "- What is DStream?\n",
    "    - It is a key abstraction, which represents a stream of data divided into small batches.\n",
    "\n",
    "\n",
    "- On what data type DStream is based on?\n",
    "    - RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80022b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "os.environ[\"SPARK_HOME\"] = \"/mnt/d/Ubuntu/Programs/spark-3.1.1-bin-hadoop3.2\"\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"/usr/bin/python3\"\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = \"python3\"\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = \"pyspark-shell\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d0b0ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext(\"local[2]\", \"NetworkWordCount\")\n",
    "ssc = StreamingContext(sc, 1)\n",
    "ssc.checkpoint('./checkpoints')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b9e7da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DStream that will connect to hostname:port, like localhost:9999\n",
    "lines = ssc.socketTextStream(\"localhost\", 9999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47afaa3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split each line into words\n",
    "words = lines.flatMap(lambda line: line.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57e815e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Window Operations\n",
    "\n",
    "# Count each word in each batch\n",
    "pairs = words.map(lambda word: (word, 1))\n",
    "\n",
    "# Reduce last 30 seconds of data, every 10 seconds\n",
    "windowedWordCounts = pairs.reduceByKeyAndWindow(lambda x, y: x + y, lambda x, y: x - y, 30, 10)\n",
    "\n",
    "# Print the first ten elements of each RDD generated in this DStream to the console\n",
    "windowedWordCounts.pprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e99e391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2021-05-13 16:36:12\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-05-13 16:36:22\n",
      "-------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ssc.start()  # Start the computation\n",
    "ssc.awaitTermination(timeout=30)  # Wait for the computation to terminate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d82a1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()\n",
    "ssc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117a4127",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
